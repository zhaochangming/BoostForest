# BoostForest

**BoostForest** [1] is an ensemble learning approach that bases on model tree [2], boosting [3] and bootstrap aggregating (Bagging) [4]. It is designed to be efficient with the following advantages:
- 
- Support of classification and regression in supervised learning.
- Support of achieving better generalization performance than traditional tree-based ensemble learning approaches.

## Get Started and Documentation

Our primary documentation is at https://boostforest.readthedocs.io and is generated from this repository. If you are new to BoostForest, follow [the installation instructions](https://boostforest.readthedocs.io/en/latest/Python-Intro.html) on that site. 

Next you may want to read:
- [**APIs & Parameters**](https://boostforest.readthedocs.io/en/latest/BoostForest.html) is an exhaustive list of customization you can make.
- [**Parameters Tuning**](https://boostforest.readthedocs.io/en/latest/Parameters-Tuning.html) is an exhaustive list of customization you can make.
- [**Examples**](https://boostforest.readthedocs.io/en/latest/Demo.html) showing command line usage of common tasks.

## References

[1] C. Zhao, D. Wu, J. Huang, Y. Yuan, H. Zhang, R. Peng and Z. Shi, “BoostTree and BoostForest for ensemble learning,” IEEE Trans. on Pattern Analysis and Machine Intelligence, 2022, in press.

[2] Y. Wang and I. H. Witten, “Induction of model trees for predicting continuous classes,” in Proc. 9th European Conf. on Machine Learning, Prague, Czech Republic, 1997.

[3] J. H. Friedman, “Greedy function approximation: A gradient boosting machine,” Annals of Statistics, vol. 29, no. 5, pp. 1189–1232, 2001.

[4] L. Breiman, “Bagging predictors,” Machine Learning, vol. 24, no. 2, pp. 123–140, 1996.
